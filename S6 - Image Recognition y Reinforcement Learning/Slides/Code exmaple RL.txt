import gym

# Create the environment
env = gym.make('CartPole-v0')

# Set the number of actions and states
num_actions = env.action_space.n
num_states = env.observation_space.shape[0]

# Initialize the Q-table to all zeros
Q = np.zeros((num_states, num_actions))

# Set the learning rate and discount factor
alpha = 0.1
gamma = 0.99

# Set the number of episodes to run
num_episodes = 1000

# Loop over the episodes
for i in range(num_episodes):
    # Reset the environment at the start of each episode
    state = env.reset()

    # Set the initial reward to 0
    reward = 0

    # Loop until the episode is done
    done = False
    while not done:
        # Choose an action using an epsilon-greedy policy
        action = np.argmax(Q[state,:] + np.random.randn(1, num_actions) * (1./(i+1)))

        # Take the action and observe the result
        next_state, reward, done, _ = env.step(action)

        # Update the Q-table using the Q-learning update rule
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state,:]) - Q[state, action])

        # Set the new state as the current state
        state = next_state

# Print the final Q-table
print(Q)

This code creates a reinforcement learning agent that learns to balance a pole on a cart using the Q-learning algorithm. The agent receives a reward for each timestep that the pole remains balanced, and the goal is to maximize the cumulative reward over time. The agent learns by taking actions, observing the result, and updating its internal model of the environment (the Q-table) based on the reward received.
